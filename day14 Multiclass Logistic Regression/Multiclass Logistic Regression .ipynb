{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize(X,Y):\n",
    "    W = np.random.randn(Y.shape[0],X.shape[0])\n",
    "    b = np.zeros((Y.shape[0],1))\n",
    "    return W,b\n",
    "\n",
    "def softmax_activation(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z),axis=0)\n",
    "\n",
    "def forward_prop(X, W, b, activationfn=softmax_activation):\n",
    "    Z = np.dot(W,X)+b\n",
    "    A = activationfn(Z)\n",
    "    return (A,Z)\n",
    "\n",
    "def compute_cost(A, Y, W, lambd):\n",
    "    m = Y.shape[1]\n",
    "    J = (-1/m) * np.sum(np.multiply(np.log(A),Y)) + ((lambd/(2*m)) * np.sum(np.square(W)))\n",
    "    return J\n",
    "\n",
    "def back_prop(X, Y, forward_cache):\n",
    "    m = X.shape[1]\n",
    "    A,Z = forward_cache\n",
    "    dW = (-1/m) * np.dot((Y-A),X.T)\n",
    "    db = (-1/m) * np.sum((Y-A),axis=1)\n",
    "    db = db.reshape(db.shape[0],1)\n",
    "    return (dW,db)\n",
    "\n",
    "def update_params(W, b, backward_cache, alpha, lambd, m):\n",
    "    dW,db = backward_cache\n",
    "    W = W - alpha * (dW+(lambd/m)*W)\n",
    "    b = b - alpha * db\n",
    "    return (W,b)\n",
    "\n",
    "def train(X, Y, epochs = 1000, alpha = 0.01, lambd = 0.1, displayRate = 100):\n",
    "    W,b = initialize(X,Y)\n",
    "    costs = []\n",
    "    iters = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        #print('Iteration: ',i+1)\n",
    "        forward_cache = forward_prop(X, W, b, activationfn = softmax_activation)\n",
    "        A,Z = forward_cache\n",
    "        cost = compute_cost(A,Y,W,lambd)\n",
    "        costs.append(cost)\n",
    "        iters.append(i+1)\n",
    "        if i % displayRate==0:\n",
    "            print('Iteration ',i,' Cost: ',cost)\n",
    "        backward_cache = back_prop(X, Y, forward_cache)\n",
    "        dW,db = backward_cache\n",
    "        '''\n",
    "        print('W.shape: ',W.shape)\n",
    "        print('b.shape: ',b.shape)\n",
    "        print('A.shape: ',A.shape)\n",
    "        print('Z.shape: ',Z.shape)\n",
    "        print('dW.shape: ',dW.shape)\n",
    "        print('db.shape: ',db.shape)\n",
    "        '''\n",
    "        W,b = update_params(W, b, backward_cache, alpha, lambd, X.shape[1])\n",
    "    return (W,b,costs,iters)\n",
    "\n",
    "def predict(X, W, b, activationfn = softmax_activation):\n",
    "    A, Z = forward_prop(X, W, b, activationfn)\n",
    "    maxi = np.max(A,axis=0)\n",
    "    for i in range(len(maxi)):\n",
    "        for j in range(A.shape[0]):\n",
    "            if(A[j,i] == maxi[i]):\n",
    "                A[j,i] = 1\n",
    "            else:\n",
    "                A[j,i] = 0\n",
    "    return A\n",
    "\n",
    "def evaluate_accuracy(A, Y):\n",
    "    count = 0\n",
    "    for i in range(A.shape[1]):\n",
    "        if np.array_equal(A[:,i], Y[:,i]):\n",
    "            count+=1\n",
    "    return count/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
